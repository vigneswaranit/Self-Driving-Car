{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.0\n"
     ]
    }
   ],
   "source": [
    "print (cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing some useful packages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import imageio\n",
    "imageio.plugins.ffmpeg.download()\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Lane Detection:\n",
    "\n",
    "    1) Convert frame to grayscale\n",
    "    2) Create masks for yellow and white pixels\n",
    "    3) Apply a Gaussian smoothing\n",
    "    4) Apply a Canny edge detection\n",
    "    5) Create an additional mask to focus on the \"region of interest\" in front of the vehicle\n",
    "    6) Convert the points(i.e. pixels) in XY space to a line in Hough space\n",
    "    7) Where the lines in Hough space intersect (i.e. a point) a line exists in XY space\n",
    "    8) Using the extrema of the lines generated, create two averaged line s 9) Create two averaged lines across frames for a smooth video playback\n",
    "    10) Draw the lines to each frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Grayscale Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grayscale(img):\n",
    "    \"\"\"Applies the Grayscale transform\n",
    "    This will return an image with only one color channel\n",
    "    but NOTE: to see the returned image as grayscale\n",
    "    (assuming your grayscaled image is called 'gray')\n",
    "    you should call plt.imshow(gray, cmap='gray')\"\"\"\n",
    "    \n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Or use BGR2GRAY if you read an image with cv2.imread()\n",
    "    # return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canny Edge Detector \n",
    "\n",
    "The Canny edge detector is an edge detection operator that uses a multi-stage algorithm to detect a wide range of edges in images. It was developed by John F. Canny in 1986. Canny also produced a computational theory of edge detection explaining why the technique works.\n",
    "\n",
    "The Process of Canny edge detection algorithm can be broken down to 5 different steps:\n",
    "\n",
    " 1. Apply Gaussian filter to smooth the image in order to remove the noise\n",
    " 2. Find the intensity gradients of the image\n",
    " 3. Apply non-maximum suppression to get rid of spurious response to edge detection\n",
    " 4. Apply double threshold to determine potential edges\n",
    " 5. Track edge by hysteresis: Finalize the detection of edges by suppressing all the other edges that are weak and not connected to strong edges.\n",
    "\n",
    "For more details look here : https://en.wikipedia.org/wiki/Canny_edge_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def canny(img, low_threshold, high_threshold):\n",
    "    \"\"\"Applies the Canny transform\"\"\"\n",
    "    return cv2.Canny(img, low_threshold, high_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Blur Filter\n",
    "The Gaussian blur is a type of image-blurring filter that uses a Gaussian function (which also expresses the normal distribution in statistics) for calculating the transformation to apply to each pixel in the image.\n",
    "\n",
    "For more details look here : https://en.wikipedia.org/wiki/Gaussian_blur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian_blur(img, kernel_size):\n",
    "    \"\"\"Applies a Gaussian Noise kernel\"\"\"\n",
    "    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def region_of_interest(img, vertices):\n",
    "    \"\"\"\n",
    "    Applies an image mask.\n",
    "    \n",
    "    Only keeps the region of the image defined by the polygon\n",
    "    formed from `vertices`. The rest of the image is set to black.\n",
    "    \"\"\"\n",
    "    #defining a blank mask to start with\n",
    "    mask = np.zeros_like(img)   \n",
    "    \n",
    "    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
    "    if len(img.shape) > 2:\n",
    "        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "        \n",
    "    #filling pixels inside the polygon defined by \"vertices\" with the fill color    \n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "    \n",
    "    #returning the image only where mask pixels are nonzero\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Slope for two points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_slope(x1,y1,x2,y2):\n",
    "    return (y2-y1)/(x2-x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hough Lines\n",
    "\n",
    "The Hough transform is a feature extraction technique used in image analysis, computer vision, and digital image processing.[1] The purpose of the technique is to find imperfect instances of objects within a certain class of shapes by a voting procedure. This voting procedure is carried out in a parameter space, from which object candidates are obtained as local maxima in a so-called accumulator space that is explicitly constructed by the algorithm for computing the Hough transform.\n",
    "\n",
    "For more Details : \n",
    "1. https://en.wikipedia.org/wiki/Hough_transform\n",
    "2. https://www.youtube.com/watch?v=4zHbI-fFIlI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n",
    "    \"\"\"\n",
    "    `img` should be the output of a Canny transform.\n",
    "        \n",
    "    Returns an image with hough lines drawn.\n",
    "    \"\"\"\n",
    "    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n",
    "    line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
    "    draw_lines(line_img,lines)\n",
    "    return line_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw lines  (Blue Lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#thick blue lines \n",
    "def draw_lines(img, lines, color=[0, 0, 255], thickness=6):\n",
    "    \"\"\"workflow:\n",
    "    1) examine each individual line returned by hough & determine if it's in left or right lane by its slope\n",
    "    because we are working \"upside down\" with the array, the left lane will have a negative slope and right positive\n",
    "    2) track extrema\n",
    "    3) compute averages\n",
    "    4) solve for b intercept \n",
    "    5) use extrema to solve for points\n",
    "    6) smooth frames and cache\n",
    "    \"\"\"\n",
    "    global cache\n",
    "    global first_frame\n",
    "    y_global_min = img.shape[0] #min will be the \"highest\" y value, or point down the road away from car\n",
    "    y_max = img.shape[0]\n",
    "    l_slope, r_slope = [],[]\n",
    "    l_lane,r_lane = [],[]\n",
    "    det_slope = 0.4\n",
    "    α =0.2 \n",
    "    #i got this alpha value off of the forums for the weighting between frames.\n",
    "    #i understand what it does, but i dont understand where it comes from\n",
    "    #much like some of the parameters in the hough function\n",
    "    \n",
    "    for line in lines:\n",
    "        #1\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            slope = get_slope(x1,y1,x2,y2)\n",
    "            if slope > det_slope:\n",
    "                r_slope.append(slope)\n",
    "                r_lane.append(line)\n",
    "            elif slope < -det_slope:\n",
    "                l_slope.append(slope)\n",
    "                l_lane.append(line)\n",
    "        #2\n",
    "        y_global_min = min(y1,y2,y_global_min)\n",
    "    \n",
    "    # to prevent errors in challenge video from dividing by zero\n",
    "    if((len(l_lane) == 0) or (len(r_lane) == 0)):\n",
    "        print ('no lane detected')\n",
    "        return 1\n",
    "        \n",
    "    #3\n",
    "    l_slope_mean = np.mean(l_slope,axis =0)\n",
    "    r_slope_mean = np.mean(r_slope,axis =0)\n",
    "    l_mean = np.mean(np.array(l_lane),axis=0)\n",
    "    r_mean = np.mean(np.array(r_lane),axis=0)\n",
    "    \n",
    "    if ((r_slope_mean == 0) or (l_slope_mean == 0 )):\n",
    "        print('dividing by zero')\n",
    "        return 1\n",
    "    \n",
    "   \n",
    "\n",
    "    #4, y=mx+b -> b = y -mx\n",
    "    l_b = l_mean[0][1] - (l_slope_mean * l_mean[0][0])\n",
    "    r_b = r_mean[0][1] - (r_slope_mean * r_mean[0][0])\n",
    "    \n",
    "    #5, using y-extrema (#2), b intercept (#4), and slope (#3) solve for x using y=mx+b\n",
    "    # x = (y-b)/m\n",
    "    # these 4 points are our two lines that we will pass to the draw function\n",
    "    l_x1 = int((y_global_min - l_b)/l_slope_mean) \n",
    "    l_x2 = int((y_max - l_b)/l_slope_mean)   \n",
    "    r_x1 = int((y_global_min - r_b)/r_slope_mean)\n",
    "    r_x2 = int((y_max - r_b)/r_slope_mean)\n",
    "    \n",
    "    #6\n",
    "    if l_x1 > r_x1:\n",
    "        l_x1 = int((l_x1+r_x1)/2)\n",
    "        r_x1 = l_x1\n",
    "        l_y1 = int((l_slope_mean * l_x1 ) + l_intercept)\n",
    "        r_y1 = int((r_slope_mean * r_x1 ) + r_intercept)\n",
    "        l_y2 = int((l_slope_mean * l_x2 ) + l_intercept)\n",
    "        r_y2 = int((r_slope_mean * r_x2 ) + r_intercept)\n",
    "    else:\n",
    "        l_y1 = y_global_min\n",
    "        l_y2 = y_max\n",
    "        r_y1 = y_global_min\n",
    "        r_y2 = y_max\n",
    "      \n",
    "    current_frame = np.array([l_x1,l_y1,l_x2,l_y2,r_x1,r_y1,r_x2,r_y2],dtype =\"float32\")\n",
    "    \n",
    "    if first_frame == 1:\n",
    "        next_frame = current_frame        \n",
    "        first_frame = 0        \n",
    "    else :\n",
    "        prev_frame = cache\n",
    "        next_frame = (1-α)*prev_frame+α*current_frame\n",
    "             \n",
    "    cv2.line(img, (int(next_frame[0]), int(next_frame[1])), (int(next_frame[2]),int(next_frame[3])), color, thickness)\n",
    "    cv2.line(img, (int(next_frame[4]), int(next_frame[5])), (int(next_frame[6]),int(next_frame[7])), color, thickness)\n",
    "    \n",
    "    cache = next_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weighted_img(img, initial_img, α=0.8, β=1., λ=0.):\n",
    "    \"\"\"\n",
    "    `img` is the output of the hough_lines(), An image with lines drawn on it.\n",
    "    Should be a blank image (all black) with lines drawn on it.\n",
    "    \n",
    "    `initial_img` should be the image before any processing.\n",
    "    \n",
    "    The result image is computed as follows:\n",
    "    \n",
    "    initial_img * α + img * β + λ\n",
    "    NOTE: initial_img and img must be the same shape!\n",
    "    \"\"\"\n",
    "    return cv2.addWeighted(initial_img, α, img, β, λ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on Images\n",
    "Run on Single Stil frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    \n",
    "    global first_frame\n",
    "\n",
    "    #Convert to grayscale Image to detect the White Lanes\n",
    "    gray_image = grayscale(image)\n",
    "    mpimg.imsave(\"out_images/01_gray_\"+source_img,gray_image)\n",
    "    \n",
    "    #Convert to HSV Image to detect the Yellow Lanes\n",
    "    img_hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    mpimg.imsave(\"out_images/02_img_hsv_\"+source_img,img_hsv)\n",
    "    \n",
    "    #hsv = [hue, saturation, value]\n",
    "    #more accurate range for yellow since it is not strictly black, white, r, g, or b\n",
    "\n",
    "    # Apply mask to the Original Image and Look for the lane pixels\n",
    "    lower_yellow = np.array([20, 100, 100], dtype = \"uint8\")\n",
    "    upper_yellow = np.array([30, 255, 255], dtype=\"uint8\")\n",
    "\n",
    "    mask_yellow = cv2.inRange(img_hsv, lower_yellow, upper_yellow)\n",
    "    mask_white = cv2.inRange(gray_image, 200, 255)\n",
    "    mask_yw = cv2.bitwise_or(mask_white, mask_yellow)\n",
    "    mask_yw_image = cv2.bitwise_and(gray_image, mask_yw)    \n",
    "    mpimg.imsave(\"out_images/03_img_masked_\"+source_img,mask_yw_image)\n",
    "\n",
    "    #Apply Gaussian Blur\n",
    "    kernel_size = 5\n",
    "    gauss_gray = gaussian_blur(mask_yw_image,kernel_size)\n",
    "    mpimg.imsave(\"out_images/04_img_Gaussian_Blur_\"+source_img,gauss_gray)\n",
    "\n",
    "    #Canny Edge Detection with the Ratio 1:2 or 1:3\n",
    "    low_threshold = 50\n",
    "    high_threshold = 150\n",
    "    canny_edges = canny(gauss_gray,low_threshold,high_threshold)\n",
    "    mpimg.imsave(\"out_images/05_img_Canny_Edge_\"+source_img,canny_edges)\n",
    "\n",
    "    #Region of Interest for the Lane Detection, We are in the centre of the Image, \n",
    "    #So omit the left and right side of the image in top as well as bottom.\n",
    "    imshape = image.shape\n",
    "    lower_left = [imshape[1]/9,imshape[0]]\n",
    "    lower_right = [imshape[1]-imshape[1]/9,imshape[0]]\n",
    "    top_left = [imshape[1]/2-imshape[1]/8,imshape[0]/2+imshape[0]/10]\n",
    "    top_right = [imshape[1]/2+imshape[1]/8,imshape[0]/2+imshape[0]/10]\n",
    "    vertices = [np.array([lower_left,top_left,top_right,lower_right],dtype=np.int32)]\n",
    "    roi_image = region_of_interest(canny_edges, vertices)\n",
    "    mpimg.imsave(\"out_images/06_img_ROI_\"+source_img,roi_image)\n",
    "\n",
    "    #rho and theta are the distance and angular resolution of the grid in Hough space\n",
    "    #same values as quiz\n",
    "    rho = 4\n",
    "    theta = np.pi/180\n",
    "    #threshold is minimum number of intersections in a grid for candidate line to go to output\n",
    "    threshold = 30\n",
    "    min_line_len = 100\n",
    "    max_line_gap = 180\n",
    "    #my hough values started closer to the values in the quiz, but got bumped up considerably for the challenge video\n",
    "    line_image = hough_lines(roi_image, rho, theta, threshold, min_line_len, max_line_gap)\n",
    "    mpimg.imsave(\"out_images/07_after_hough_transform\"+source_img,line_image)\n",
    "    \n",
    "    result = weighted_img(line_image, image, α=0.8, β=1., λ=0.)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for source_img in os.listdir(\"test_images/\"):\n",
    "    first_frame = 1\n",
    "    image = mpimg.imread(\"test_images/\"+source_img)\n",
    "    processed = process_image(image)\n",
    "    mpimg.imsave(\"out_images/99_annotated_\"+source_img,processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video white.mp4\n",
      "[MoviePy] Writing video white.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████▋| 221/222 [01:44<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: white.mp4 \n",
      "\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "first_frame = 1\n",
    "white_output = 'white.mp4'\n",
    "clip1 = VideoFileClip(\"solidWhiteRight.mp4\")\n",
    "white_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "%time white_clip.write_videofile(white_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"white.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(white_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video yellow.mp4\n",
      "[MoviePy] Writing video yellow.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████▉| 681/682 [05:15<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: yellow.mp4 \n",
      "\n",
      "Wall time: 5min 15s\n"
     ]
    }
   ],
   "source": [
    "first_frame = 1\n",
    "yellow_output = 'yellow.mp4'\n",
    "clip2 = VideoFileClip('solidYellowLeft.mp4')\n",
    "yellow_clip = clip2.fl_image(process_image)\n",
    "%time yellow_clip.write_videofile(yellow_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"yellow.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(yellow_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video extra.mp4\n",
      "[MoviePy] Writing video extra.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████████████████████████████████████████████████                              | 157/251 [02:50<01:28,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no lane detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|██████████████████████████████████████████████████▎                             | 158/251 [02:51<01:27,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no lane detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|██████████████████████████████████████████████████▋                             | 159/251 [02:52<01:25,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no lane detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 251/251 [04:25<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: extra.mp4 \n",
      "\n",
      "Wall time: 4min 26s\n"
     ]
    }
   ],
   "source": [
    "first_frame = 1\n",
    "challenge_output = 'extra.mp4'\n",
    "clip2 = VideoFileClip('challenge.mp4')\n",
    "challenge_clip = clip2.fl_image(process_image)\n",
    "%time challenge_clip.write_videofile(challenge_output, audio=False)\n",
    "#TODO how do we make curved ROI and curved lines? we need some calculus up in this bizzzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"extra.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(challenge_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
